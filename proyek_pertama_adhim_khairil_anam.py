# -*- coding: utf-8 -*-
"""Proyek Pertama_Adhim Khairil Anam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11I7lhyXU7ioS_Cc6v5oIdRbPvNzjv3FZ

# **Predictive Analytics: Medical Insurance Premium Prediction**

# **1. Import/Install Library yang Dibutuhkan**
"""

!pip install -q gdown

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import zipfile
from sklearn.model_selection import train_test_split
# %matplotlib inline

"""# **2. Data Understanding**

# Data Collection
"""

!gdown --id "1583goh5FQOTclA1U3SoVVO9hvnp5miZY"

from google.colab import files
#Upload Kaggle API Key
files.upload()

"""# **Exploratory Data Analysis (EDA)**"""

df = pd.read_csv("/content/submissiondataset.zip")
df.head(10)

"""# Data Understanding & Removing Outlier

"""

df.shape

df.info()

df.describe()

"""Fungsi describe() memberikan informasi statistik pada masing-masing kolom, antara lain:

- Count adalah jumlah sampel pada data.
- Mean adalah nilai rata-rata.
- Std adalah standar deviasi.
- Min yaitu nilai minimum setiap kolom
- Max adalah nilai maksimum

# Handling Missing Value
"""

df.isnull().sum()

"""Kolom yang digunakan untuk modeling tidak memiliki missing values, sehingga tidak diperlukan imputasi"""

df.isnull().sum().sum()

df.shape

"""Jumlah Datasets setalah penanganan Missing Value dan Outlier: 986, 11

# Handling Outliers
"""

# Hitung IQR (Interquartile Range)
Q1 = df['Age'].quantile(0.25)
Q3 = df['Age'].quantile(0.75)
IQR = Q3 - Q1

# Tentukan batas bawah dan atas
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print(f"Batas bawah: {lower_bound}, Batas atas: {upper_bound}")

# Deteksi outliers
outliers = df[(df['Age'] < lower_bound) | (df['Age'] > upper_bound)]
print(f"Jumlah data outlier pada kolom Age: {len(outliers)}")

# Handling: Buang outliers (opsional: kamu bisa juga imputasi)
df_no_outliers = df[(df['Age'] >= lower_bound) & (df['Age'] <= upper_bound)]

"""**Frekuensi Umur**"""

sns.boxplot(x=df['Age'])

"""****Frekuensi Tinggi Badan****"""

sns.boxplot(x=df['Height'])

"""**Frekuensi Berat Badan**"""

sns.boxplot(x=df['Weight'])

"""**Frekuensi Jumlah Operasi Besar**"""

sns.boxplot(x=df['NumberOfMajorSurgeries'])

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR=Q3-Q1
insurance=df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]

# Cek ukuran dataset setelah kita drop outliers
insurance.shape

"""Jumlah Datasets setalah kita drop Outlier: 518, 11

**EDA - Univariate Analysis**

Univariate analysis dilakukan untuk memahami distribusi masing-masing fitur:
"""

cat_features = ['Diabetes', 'BloodPressureProblems', 'AnyTransplants', 'AnyChronicDiseases', 'KnownAllergies', 'HistoryOfCancerInFamily']
num_features = ['Age', 'Height', 'Weight', 'NumberOfMajorSurgeries', 'PremiumPrice']

plt.subplots(2, 2, figsize=(20, 16))

for i, col in enumerate(cat_features):
  plt.subplot(2, 3, i + 1)
  df.groupby(col).size().plot(kind='bar', rot=0)

feature = cat_features[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df0 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df0)
count.plot(kind='bar', title=feature);

feature = cat_features[1]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df1 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df1)
count.plot(kind='bar', title=feature);

feature = cat_features[2]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df2 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df2)
count.plot(kind='bar', title=feature);

feature = cat_features[3]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df3 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df3)
count.plot(kind='bar', title=feature);

feature = cat_features[4]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df4 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df4)
count.plot(kind='bar', title=feature);

feature = cat_features[5]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df5 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df5)
count.plot(kind='bar', title=feature);

"""**Numerical Features**"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""**2.2.4 EDA - Multivariate Analysis**

**Categorical Features**
"""

cat_features = df.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="PremiumPrice", kind="bar", dodge=False, height = 4, aspect = 3,  data=df, palette="Set3")
  plt.title("Rata-rata 'PremiumPrice' Relatif terhadap - {}".format(col))

"""**Numerical Features**"""

sns.pairplot(df, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)

# Untuk print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='PiYG' )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""**Insight :** Fitur Age adalah prediktor paling dominan untuk PremiumPrice, diikuti oleh faktor medis seperti AnyTransplants, NumberOfMajorSurgeries, dan AnyChronicDiseases. Fitur seperti Height, Weight, dan KnownAllergies memiliki pengaruh yang sangat kecil dan bisa dipertimbangkan untuk dieliminasi pada tahap seleksi fitur, tergantung kebutuhan model.

# Data Preparation
"""

#One Hot Encoding
data_cat =  pd.get_dummies(df[['Diabetes', 'BloodPressureProblems', 'AnyTransplants', 'AnyChronicDiseases', 'KnownAllergies', 'HistoryOfCancerInFamily']])

# Concenate Dataframe
df = pd.concat([df,data_cat],axis=1)

df.drop(columns=['Diabetes', 'BloodPressureProblems', 'AnyTransplants', 'AnyChronicDiseases', 'KnownAllergies', 'HistoryOfCancerInFamily'],inplace=True)

# Split Dataset
from sklearn.model_selection import train_test_split

X = df.drop(['PremiumPrice'],axis =1)
y = df['PremiumPrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

print(f'Total # of sample in whole dataset: {len(y)}')
print(f'Total # of sample in train dataset: {len(y_train)}')
print(f'Total # of sample in test dataset: {len(y_test)}')

"""# Model Development"""

# Dataframe untuk menyimpan hasil
result = pd.DataFrame(index=['train_mse', 'test_mse','eval_train','eval_test'],
                      columns=['Huber', 'SVR'])

from sklearn.linear_model import HuberRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

# Train Model Huber
hr = HuberRegressor()
hr.fit(X_train, y_train)

result.loc['train_mse','Huber'] = mean_squared_error(y_pred = hr.predict(X_train), y_true=y_train)

# Train Model SVR
svr = SVR()
svr.fit(X_train, y_train)

result.loc['train_mse','SVR'] = mean_squared_error(y_pred = svr.predict(X_train), y_true=y_train)

# Prediksi data testing dan simpan hasil ke dataframe
result.loc['test_mse','Huber'] = mean_squared_error(y_pred = hr.predict(X_test), y_true=y_test)
result.loc['test_mse','SVR'] = mean_squared_error(y_pred = svr.predict(X_test), y_true=y_test)

result

"""# Model Evaluation"""

result.plot(kind='bar')

from sklearn.model_selection import GridSearchCV

# Hyperparams Menggunakan Grid Search pada Huber
hr_eval = HuberRegressor()
param_grid = { #Setup Params
    'epsilon': [1.0, 1.5, 2.0],
    'alpha': [0.0001, 0.001, 0.01],
    'max_iter': [100, 200, 300]
}

grid_search_huber = GridSearchCV(hr_eval, param_grid, scoring='neg_mean_squared_error', cv=5)

# Train Model dengan hyperparam
grid_search_huber.fit(X_train, y_train)

print("Best hyperparameters:", grid_search_huber.best_params_)
print("Best Score:", grid_search_huber.best_score_)

# Data Testing
result.loc['eval_train','Huber'] = mean_squared_error(y_pred = grid_search_huber.predict(X_train), y_true=y_train)
result.loc['eval_test','Huber'] = mean_squared_error(y_pred = grid_search_huber.predict(X_test), y_true=y_test)

# Hyperparams Menggunakan Grid Search pada SVR
svr_eval = SVR()
param_grid_svr = { #Setup Params
    'kernel': ['linear', 'rbf'],
    'C': [0.1, 1, 10],
    'epsilon': [0.1, 0.2, 0.3]
}
# Train models menggunakan hyperparams
grid_search_svr = GridSearchCV(svr_eval, param_grid_svr, scoring='neg_mean_squared_error', cv=5)
grid_search_svr.fit(X_train, y_train)

print("Best hyperparameters:", grid_search_svr.best_params_)
print("Best Score:", grid_search_svr.best_score_)

# Data Testing
result.loc['eval_train','SVR'] = mean_squared_error(y_pred = grid_search_svr.predict(X_train), y_true=y_train)
result.loc['eval_test','SVR'] = mean_squared_error(y_pred = grid_search_svr.predict(X_test), y_true=y_test)

result

result.plot(kind='bar')

"""**Insight dari Evaluasi Model Prediksi**
1. Model dengan Performa Terbaik:
  - Huber Regressor

  - Train MSE: Lebih rendah dari SVR menunjukkan bahwa Huber lebih baik dalam menyesuaikan data pelatihan.

  - Test MSE: Secara signifikan lebih rendah dari SVR menunjukkan generalisasi yang lebih baik ke data uji.

  - Eval_train dan Eval_test: Huber tetap lebih rendah dari SVR di kedua metrik ini (asumsinya bisa R² error atau metrik evaluasi lain seperti MAE).

2. Tingkat Error (MSE dan Evaluasi Tambahan):
Nilai tidak disebutkan secara numerik, namun bisa diinterpretasikan relatif dari grafik.

- SVR (Support Vector Regressor):

  - Memiliki MSE tertinggi pada data latih dan uji → overfitting kemungkinan besar tidak terjadi, tetapi SVR memiliki fit yang kurang baik secara umum.

  - Semua nilai error SVR berada di atas Huber.

- Huber Regressor:

  - Konsisten menghasilkan error yang lebih kecil di semua metrik, menunjukkan robustness terhadap outlier dan kestabilan dalam prediksi.


"""